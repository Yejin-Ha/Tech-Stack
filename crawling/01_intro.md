# 웹크롤링 & 스크래핑
## 필요 기술셋
- html
- css
- javascript
- 데이터 수집 기술 - python, non-python
- python
  - 데이터 수집 library 다양
  - 동적 자바 스크립트 기반의 데이터 수집
    - 브라우저 드라이버등도 필요
  
## 크롤링과 스크래핑 비교
- 크롤링
  - 웹 페이지의 하이퍼링크를 순회하면서 웹 페이지를 다운로드 하는 작업
  - 웹 사이트를 정기적으로 돌며 정보를 추출하는 기술 의미
  - (저앻진 시간대에 또는 저앻진 이벤트 발생시 자동 실행.. 기술이 적용)
  - 크론 & 스케쥴러 : 정해진 시간에 업무 자동화처럼 작업을 수행
- 스크래핑
  - 다운로드한 웹 페이지에서 필요한 특정 정보를 추출하는 기술

## 스크래핑 필요성
- 데이터 가져오기
  - 예시 : 소셜 데이터 -> 누구와 연관이 있는지
- 외부로 내보내는 기능이 없는 시스템에서 데이터 가져오기
- 사이트를 모니터링하며 새로운 정보 감지
- 검색 엔진의 DB를 구축하기 위한 스크래핑
- 고려사항 
  - 웹 페이지의 내용을 마음대로 발췌한다는 부분에서 논란의 여지가 있음
  - copyright된 정보를 출판하는 것은 허용되지 않음
  - `이용 약관을 이용하지 않도록 유의`

## 파이썬으로 크롤링 & 스크래핑
- 파이썬 기반의 작업시 장점
- 강력한 라이브러리
  - python package index(PyPI)에 수많은 개발자들이 개발한 라이브러리 공개
  - 유명한 스크래핑 라이브러리 제공
    - BeautifulSoup
    - lxml
    - Selenium
  - 스크래핑 후처리의 편리성
    - 크롤링 / 스크래핑으로 데이터를 추출한 후 뎅터 분석 등의 처리시 파이썬은 강력한 무기로 작용함
```
pip list
```
<br>
<br>
<br>

# 데이터 저장 방식
## 1. 크롤링한 데이터 저장 형식
- 크롤링한 데이터를 파일 등에 저장해 두면 데이터를 쉽게 활용할 수 있음
- RESTAPI, ELK

## 2. CSV 형식으로 저장하기 
- comma - seperated values
  - 하나의 recode를 한 줄로 ㅍ현

## 3. JSON 형식으로 저장하기 


## Oracle DB 연동의 위한 기초
- RDBMS에 저장된 데이터들을 DataFrame으로 생성 가능
  - 데이터 전처리 쉽게 수행 가능
  - ML/DL등으로 서비스 로직을 사용자에게 최적화해서 개발해서 제공

<br>
<br>
<br>

# 크롤링 고려 사항
## 크롤러 구성시 주의 사항
- 저작권 고려
- 너무 많은 부하(지속적으로 계속 수집하는 작업)는 업무 방해등으로 고소당할 수 있음
  - 수집하는 user들의 접속 정보를 로그에 기록한다.
  - 1초단위/3초단위 같이 접속인 경우 비정상으로 간주해서 차단한다.
- 종량제 사용하는 서버인 경우 부하가 많이 걸리면 과금 발생
- 웹 사이트들의 robots.txt 확인하기
    ```
    https://www.naver.com/robots.txt
    ```

## 크롤러와 URL
- 크롤러
  - 웹 페이지에 존재하는 하이퍼링크를 따라 돌아야한다.
- 크롤러와 URL
  - 브라우저에서는 링크를 클릭하면 되지만 크롤러로 링크를 돌아다니면 URL과 관련된 기초적인 지식을 이해하고 활용해야 한다.
- URL 구조
  1. schema
  2. authority
  3. path
  4. web query
