# 선형회귀 개요

선형 회귀(線型回歸, Linear regression)는 종속 변수 y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법. [위키백과](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80)

## 1. 선형회귀 모델
$$
\hat{y_i} = w_1 x_{i1} + w_2 x_{i2}... + w_{p} x_{ip} + b
$$


- $\hat{y_i}$: 예측값
- $x$: 특성(feature-컬럼)
- $w$: 가중치(weight), 회귀계수(regression coefficient). 특성이 $\hat{y_i}$ 에 얼마나 영향을 주는지 정도
- $b$: 절편
- $p$: p 번째 특성(feature)/p번째 가중치
- $i$: i번째 관측치(sample)

### 1. LinearRegression
- 가장 기본적인 선형 회귀 모델
- Feature 전처리
    - 범주형 : one hot encoding
    - 연속형 : Feature Scaling 한다.(StandartdScaler를 사용하는 경우에 성능이 더 잘나오는 경향이 있다.)
- 메소드
    - fit(X, y)
    - predict(x) : 예측 값 출력
    - intercept_ : 편향(절편, bias) 출력
    - coef_ : 가중치(weight) 출력
<details>
<summary>예시 코드 보기</summary>

```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train_scaled, y_train) # 각 feature에 곱할 weight(가중치), bias(절편)을 찾는다.

# 편향(절편, bias)
print(lr.intercept_)
# 가중치(weight)
print(lr.coef_)
```
</details>

### 2. 다항회귀(Polynomial Regression)
- 단순한 직선형 보다는 복잡한 비선형 형태의 데이터를 추론하기 위한 모델
- feature들을 거듭제곱한 Feature들을 추가하여 모델링한다.
- polynomialFeatures 변환기를 이용
- 속성
  - degree : 최고차항을 설정한다.
  - include_bias : True시 상수항(1) 추가(모든 값이 1인 feature(컬럼) 추가)
- 메소드
  - fit_transform(x) 
  - fit(X, y)
  - transform(X)
  - predict(x) 
  - coef_ : 가중치(weight) 출력
  - get_feature_names() : 다항회귀로 생성된 feature의 이름을 출력
<details>
<summary>예시 코드 보기</summary>

```python
from sklearn.preprocessing import PolynomialFeatures

# 모델 생성
pnf = PolynomialFeatures(degree=2, include_bias=True)
X_poly = pnf.fit_transform(X)
```
</details>
