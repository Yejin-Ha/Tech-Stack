# 1. 모델링 결과
1. 과소적합 Underfitting
2. 일반화 Generalization
    - 모델이 test set에 대하여 정확히 예측한 것
    - 모델이 train set의 결과와 test set의 결과의 차이가 거의 없는 좋은 평가지표 
3. 과대적합 Overfitting


### 1. 과대적합 Overfitting
- 데이터에 비해 모델이 너무 복잡한 경우 발생
    - 모델을 단순하게 만들면 된다.
        - 데이터 양을 늘린다. => 시간과 돈이 들기 때문에 현실적인 어려움 존재
        - 각 모델의 하이퍼 파라미터를 조절한다.
- 모델이 train set 에서는 성능이 너무 좋지만 test set에서는 성능이 좋지 않은 것
- 모델이 train set에 너무 맞춰서 학습된 경우에 새로운 데이터에 대한 성능이 떨어져 발생한다.

### 2. 과소적합 Underfitting
- 데이터 양에 비해 모델이 너무 단순한 경우 
- 모델이 train, test set 모두에서 성능이 안좋은 것
- 충분히 학습하지 못하여 데이터 셋의 패턴들을 다 찾지 못한 상태
  
# 2. Decision Tree 복잡도 제어하기
- 모델을 복잡하게 하는 것은 노드가 너무 많이 만들어 지는 것이다. => 질문이 많아진다.
  - 노드가 많아질수록 train set에 overfitting 된다.
  - 그러므로 **적절한 시점에 트리 생성을 중단**해야 한다. 
- 복잡도 관련 하이퍼 파라미터
  - max_depth : 트리의 최대 깊이
  - max_leaf_nodes : leaf node(잎 노드) 개수
    - leaf node : 트리 전체의 자식 노드의 수
  - min_samples_leaf : leaf node가 되기 위한 최소 샘플 수
  - min_samples_split : 나누는 최소 샘플 수

> 하이퍼 파라미터 Hyper Parameter - 모델의 학습에 영향을 끼치는 파라미터 값으로 모델 생성시 사람이 직접 지정해줘야 하는 값  
> 하이퍼 파라미터 튜닝 Hyper Parameter Tunning - 가장 성능 좋은 하이퍼 파라미터를 찾는 것   
> 파라미터 Parameter - 머신러닝 모델에서 파라미터는 모델이 학습을 통해서 직접 찾아야 하는 값

# 3. 최적의 하이퍼 파라미터 찾기
- 방법
  1. 만족할 만한 하이퍼 파라미터들의 값의 조합을 찾을 때 까지 일일이 수동으로 조정
  2. Grid Search 사용
     - 시도해볼 하이퍼 파라미터들을 지정하면 모든 조합에 대해 교차검증 후 제일 좋은 성능을 내는 하이퍼 파라미터 조합을 찾는다.
     - 적은 수의 조합의 경우엔 괜찮지만 시도할 하이퍼 파라미터와 값들이 많아지면 시간이 많이 걸린다. 
  3. Random Search 사용
     - Grid Search와 동일한 방식으로 사용한다.
     - 모든 조합을 다 시도하지 않고 각 반복마다 임의의 값을 대입하여 지정한 횟수만큼만 평가한다.

## 3-1. GridSearchCV()
- 주요 매개변수
  - estimator : 모델 객체 지정
  - params : 하이퍼 파라미터 목록을 dictionary로 전달
  - scoring : 평가 지표
  - cv : 검증시 fold의 개수
  - n_jobs : 사용할 CPU 코어 개수(None:1(기본값), -1:모든 코어 사용)
- 메소드
  - fit(X, y) : 학습
  - predict(X) : 제일 좋은 성능을 낸 모델로 predict() 호출
  - predict_proba(X) : 제일 좋은 성능을 낸 모델로 predict_proba() 호출
- 결과 조회 속성
- 
