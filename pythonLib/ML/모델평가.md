# 모델 평가
- 모델의 성능을 평가한다. 평가결과에 따라 프로세스를 다시 반복한다.<br>

## 분류와 회귀의 평가방법
### 분류 평가 지표
- 정확도를 제외한 모든 것들은 이진분류(다중분류)에 사용된다.
1. 정확도 (Accuracy)
2. 정밀도 (Precision)
3. 재현률 (Recall)
4. F1점수 (F1 Score)
5. PR Curve, AP
1. ROC, AUC

### 회귀 평가방법
1. MSE (Mean Squared Error)
    - 값이 작을 수록 정답에 가깝다.
    - 에러를 제곱하여 평균을 낸것
    - sklearn에서 함수를 제공한다.
1. RMSE (Root Mean Squared Error)
    - MSE에 루트를 씌운 것 => 값이 작을수록 정답에 가깝다
    - sklearn에서는 함수를 제공하지 않아서 구해야 한다.
1. $R^2$ (결정계수)
    - 모두 1에 가까울 수록 좋은 값이다.
    - 평균으로 예측한 것 보다 모델의 성능이 얼마나 좋은지를 알려준다.

### sckit-learn 평가함수 
- sklearn.metrics 모듈을 통해 제공
<br>
<br>
<br>
<br>

# 분류(Classification) 평가 기준
## 용어
- ### 이진 분류에서 양성과 음성
    - 양성(Positive): 예측하려는(찾으려는) 대상, 틀렸을 때 문제가 더 큰 대상
    - 음성(Negative): 예측하려는 대상이 아닌 것
    - 양성에 비해서 음성이 덜 중요하다.(양성은 틀리면 안된다.)
    - 그냥 A/B로 분류하는 것과는 느낌이 다르다.
    - 예
        - 암환자 분류 : 양성 - 암 환자, 음성 - 정상인
            - 양성을 음성이라고 판단할 경우 죽을 수 있음
            - 음성을 양성이라고 판단할 경우는 딱히 큰 문제가 없음 
        - 스팸메일 분류 : 양성 - 스팸메일, 음성 - 정상메일
        - 금융사기 모델: 양성 - 사기거래, 음성 - 정상거래
<br>
<br>

## 정확도 (Accuracy)
$$
\large{
정확도 (Accuracy) = \cfrac{맞게 예측한 건수} {전체 예측 건수}
}
$$
- 전체 예측 한 것중 맞게 예측한 비율로 평가한다.
- `accuracy_score(정답, 모델예측값)`
<br>

### Accuracy 평가지표의 문제
- `불균형 데이터의 경우 정확한 평가지표가 될 수 없다.`
    - 예: 양성과 음성의 비율이 1:9 인 경우 모두 음성이라고 하면 정확도는 90%가 된다.
        - 양성을 분류하지 못하기 때문에 정확도는 높지만 모델의 성능은 좋지 않은 경우가 된다.(쓸 수 없는 모델)
- 보통 데이터에는 양성보다 음성의 비율이 높다.
- 한 지표에 대해 얼마나 맞았는지 틀렸는지르 보지 못하는 단점이 있다.
<br>
<br>

## 혼동 행렬(Confusion Marix)
- 분류의 평가지표의 기준으로 사용된다.
- 혼동행렬을 이용해 다양한 평가지표(정확도, 재현률, 정밀도, F1 점수, AUC 점수)를 계산할 수 있다.
- 함수: confusion_matrix(정답, 모델예측값)
- 결과의 0번축: 실제 class, 1번 축: 예측 class

![image.png](attachment:image.png)

![img](https://skappal7.files.wordpress.com/2018/08/confusion-matrix.jpg?w=748)

- TP(True Positive) - 양성으로 예측했는데 맞은 개수
- TN(True Negative) - 음성으로 예측했는데 맞은 개수
- FP(False Positive) - 양성으로 예측했는데 틀린 개수 (음성을 양성으로 예측)
- FN(False Negative) - 음성으로 예측했는데 틀린 개수 (양성을 음성으로 예측)